{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 21.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install kfp >/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import kfp\n",
    "import kfp.compiler as compiler\n",
    "import random\n",
    "import string\n",
    "\n",
    "generate = lambda hint: \"{}-{}\".format(hint, ''.join([random.choice(string.digits) for n in range(4)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_token = os.getenv(\"DKUBE_USER_ACCESS_TOKEN\")\n",
    "client = kfp.Client(existing_token=existing_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input and output parameters for the pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = os.getenv('USERNAME')\n",
    "\n",
    "# Project owner resources\n",
    "project_id = \"ab0rj2\"\n",
    "project_name = \"titanic-demo\"\n",
    "project_owner = \"ravih1\"\n",
    "ptrain_dataset = f'titanic-train-ds:{project_owner}'\n",
    "ptest_dataset = f'titanic-test-ds:{project_owner}'\n",
    "\n",
    "# User specific resources\n",
    "train_fs = f'titanic-train-fs-{user}'\n",
    "test_fs = f'titanic-test-fs-{user}'\n",
    "training_program = 'titanic-code-user'\n",
    "model = 'titanic-model-user'\n",
    "\n",
    "# Program specific variables\n",
    "image = \"docker.io/ocdr/dkube-datascience-tf-cpu:fs-v2.0.0\"\n",
    "dataset_mount_points = [\"/opt/dkube/input/train\", \"/opt/dkube/input/test\"]\n",
    "output_featureset_mount_points = [\"/opt/dkube/output/train\", \"/opt/dkube/output/test\"]\n",
    "preprocessing_script = f\"python preprocessing.py --train_fs {train_fs} --test_fs {test_fs}\"\n",
    "training_script = \"python training.py\"\n",
    "train_inp_mount_points = [\"/titanic-train\",\"/titanic-test\"]\n",
    "train_out_mount_points = [\"/model\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_url = \"https://raw.githubusercontent.com/oneconvergence/dkube/master/components/\"\n",
    "dkube_preprocessing_op = kfp.components.load_component_from_url(components_url + \"preprocess/component.yaml\")\n",
    "dkube_training_op = kfp.components.load_component_from_url(components_url + \"training/component.yaml\")\n",
    "dkube_storage_op  = kfp.components.load_component_from_url(components_url + \"storage/component.yaml\")\n",
    "dkube_submit_op = kfp.components.load_component_from_url(components_url + \"submit/component.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name='dkube-titanic-pl',\n",
    "    description='example titanic pipeline to submit to leaderboard'\n",
    ")\n",
    "def titanic_pipeline(token, project_id):\n",
    "\n",
    "    preprocessing = dkube_preprocessing_op(token, json.dumps({\"image\": image}),\n",
    "                                           tags=json.dumps([f\"project:{project_id}\"]),\n",
    "                                           program=training_program, run_script=preprocessing_script,\n",
    "                                           datasets=json.dumps([ptrain_dataset, ptest_dataset]), \n",
    "                                           output_featuresets=json.dumps([train_fs, test_fs]),\n",
    "                                           input_dataset_mounts=json.dumps(dataset_mount_points), \n",
    "                                           output_featureset_mounts=json.dumps(output_featureset_mount_points)\n",
    "                                            )\n",
    "\n",
    "    with kfp.dsl.ExitHandler(exit_op=dkube_storage_op(\"reclaim\", token, namespace=\"kubeflow\",uid=\"{{workflow.uid}}\")):\n",
    "        input_volumes = json.dumps([\n",
    "                                    \"{{workflow.uid}}-model@model://\" + model,\n",
    "                                    \"{{workflow.uid}}-code@program://\" + training_program\n",
    "                                    ])\n",
    "        train       = dkube_training_op(token, json.dumps({\"image\": image}),\n",
    "                                        tags=json.dumps([f\"project:{project_id}\"]),\n",
    "                                        framework=\"sklearn\", version=\"0.23.2\",\n",
    "                                        program=training_program, run_script=training_script,\n",
    "                                        featuresets=json.dumps([train_fs, test_fs]), outputs=json.dumps([model]),\n",
    "                                        input_featureset_mounts=json.dumps(train_inp_mount_points),\n",
    "                                        output_mounts=json.dumps(train_out_mount_points)).after(preprocessing)\n",
    "        \n",
    "        storage  = dkube_storage_op(\"export\", token, namespace=\"kubeflow\", input_volumes=input_volumes).after(train)\n",
    "\n",
    "    \n",
    "        predict_op = kfp.dsl.ContainerOp(\n",
    "            name=\"predict\",\n",
    "            image=image,\n",
    "            command=[\"python\", \"/code/predict.py\"],\n",
    "            pvolumes={\n",
    "                     \"/model/\": kfp.dsl.PipelineVolume(pvc=\"{{workflow.uid}}-model\"),\n",
    "                     \"/code/\": kfp.dsl.PipelineVolume(pvc=\"{{workflow.uid}}-code\")\n",
    "                     },\n",
    "            file_outputs={\"output\": \"/tmp/prediction.csv\"},\n",
    "        ).after(storage)\n",
    "\n",
    "        predictions = kfp.dsl.InputArgumentPath(predict_op.outputs[\"output\"])\n",
    "\n",
    "        submit = dkube_submit_op(token, project_id, predictions=predict_op.outputs[\"output\"]).after(predict_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compling pipeline into tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_file_name = 'train_pl.tar.gz'\n",
    "pl_name = f'[{project_name}]' + f'-{user}-' + generate('pl') \n",
    "\n",
    "compiler.Compiler().compile(titanic_pipeline, pl_file_name)\n",
    "# Upload this generated tarball into the Pipelines UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Pipeline link <a href=/pipeline/#/pipelines/details/24d75203-2ab6-4084-a03d-49da210ac1d8>here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = kfp.Client(existing_token=existing_token)\n",
    "\n",
    "#pipeline_id = client.get_pipeline_id(pl_name)\n",
    "try:\n",
    "    resp = client.upload_pipeline(pipeline_package_path = pl_file_name, \n",
    "                                  pipeline_name = pl_name, description = None)\n",
    "except BaseException as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/0d99d395-4e9b-451c-bb7a-9ca01ac038f5\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = f'[{project_name}]' + f'-{user}-' + 'exp'\n",
    "try:\n",
    "    titanic_experiment = client.create_experiment(name=experiment_name)\n",
    "except BaseException as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating run from pipeline under the titanic experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/0d99d395-4e9b-451c-bb7a-9ca01ac038f5\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/4e37fea4-bbae-4bf7-850e-c2630532a301\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_name = f\"[{project_name}]\" + f'-{user}-' + generate(\"run\")\n",
    "try:\n",
    "#    pipeline_id = client.get_pipeline_id(pl_name)\n",
    "#    run = client.run_pipeline(titanic_experiment.id, run_name, None,\n",
    "#                              params={\"token\":existing_token, \"project_id\":project_id})\n",
    "    client.create_run_from_pipeline_func(titanic_pipeline, run_name=run_name, \n",
    "                                         arguments={\"token\":existing_token,\"project_id\":project_id}, experiment_name = titanic_experiment.name)\n",
    "except BaseException as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = kfp.Client(existing_token=existing_token)\n",
    "# #Replace [titanic] & wprz8s with your project name and id respectively.\n",
    "# client.create_run_from_pipeline_func(titanic_pipeline, run_name=\"[titanic] Run\" + str(runid), arguments={\"token\":existing_token,\"project_id\":\"ynli7c\"}, experiment_name = titanic_experiment.name)\n",
    "# runid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
